{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Importing\n",
    "- 이미지 파일에서 추출한 특성 embedding vector 관리 : `feature_extraction.py`\n",
    "- csv 파일 및 데이터 사용 : pandas, numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feature_extraction as fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU with cuda\n",
    "- GPU 사용여부 확인 및 사용 device 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 사용 가능한 GPU의 수: 1\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용 설정\n",
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"현재 사용 가능한 GPU의 수: {device_count}\")\n",
    "else:\n",
    "    print(\"GPU를 사용할 수 없습니다.\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEED 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(2024) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경로 확인 및 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Programming\\\\DL\\\\Anomaly_Detection\\\\code'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파일 경로 확인\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 변경\n",
    "os.chdir(os.getcwd() + '\\..\\data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setting\n",
    "- csv 형태로 저장된 데이터 로딩\n",
    "- 위에서 변경한 경로를 통해 `train.csv` 파일 로딩 및 이미지 데이터 embedding\n",
    "- 이미지 데이터에 적용할 모델 선정\n",
    "    - `torchvision` 에서 제공하는 ImageNet 기반 선행 학습 모델 로딩 : ResNet, VGG-16, VGG-19, AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로딩 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): csv 파일의 경로.\n",
    "            transform (callable, optional): 샘플에 적용될 Optional transform.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.getcwd() + self.df['img_path'].iloc[idx]\n",
    "        image = Image.open(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        target = torch.tensor([0.]).float()\n",
    "        return image, target\n",
    "\n",
    "# 이미지 전처리 및 임베딩\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_data = CustomDataset(csv_file='./train.csv', transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhsmf\\anaconda3\\envs\\ml_venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dhsmf\\anaconda3\\envs\\ml_venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\dhsmf\\anaconda3\\envs\\ml_venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\dhsmf/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17bd9d2b759492fa6dda0dcc7558e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/528M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhsmf\\anaconda3\\envs\\ml_venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to C:\\Users\\dhsmf/.cache\\torch\\hub\\checkpoints\\vgg19-dcbb9e9d.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d48c15ceb3744e8b5d695eae6e2b01b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhsmf\\anaconda3\\envs\\ml_venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to C:\\Users\\dhsmf/.cache\\torch\\hub\\checkpoints\\alexnet-owt-7be5be79.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75304c3d233a4b46a7f2080b7009b9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/233M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m vgg19_model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mvgg19(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# AlexNet 모델을 불러옵니다.\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m alexnet_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malexnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# ResNet 모델을 특징 추출기에 전달하여 특징 추출기를 초기화\u001b[39;00m\n\u001b[0;32m     11\u001b[0m resnet_feature_extractor \u001b[38;5;241m=\u001b[39m fe\u001b[38;5;241m.\u001b[39mFeatureExtractor(resnet_model)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\dhsmf\\anaconda3\\envs\\ml_venv\\lib\\site-packages\\torchvision\\models\\_utils.py:142\u001b[0m, in \u001b[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_to_str(\u001b[38;5;28mtuple\u001b[39m(keyword_only_kwargs\u001b[38;5;241m.\u001b[39mkeys()),\u001b[38;5;250m \u001b[39mseparate_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as positional \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter(s) is deprecated since 0.13 and will be removed in 0.15. Please use keyword parameter(s) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m     )\n\u001b[0;32m    140\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(keyword_only_kwargs)\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dhsmf\\anaconda3\\envs\\ml_venv\\lib\\site-packages\\torchvision\\models\\_utils.py:228\u001b[0m, in \u001b[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[0;32m    226\u001b[0m     kwargs[weights_param] \u001b[38;5;241m=\u001b[39m default_weights_arg\n\u001b[1;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m builder(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dhsmf\\anaconda3\\envs\\ml_venv\\lib\\site-packages\\torchvision\\models\\alexnet.py:114\u001b[0m, in \u001b[0;36malexnet\u001b[1;34m(weights, progress, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m model \u001b[38;5;241m=\u001b[39m AlexNet(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\dhsmf\\anaconda3\\envs\\ml_venv\\lib\\site-packages\\torchvision\\models\\_api.py:63\u001b[0m, in \u001b[0;36mWeightsEnum.get_state_dict\u001b[1;34m(self, progress)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, progress: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_state_dict_from_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dhsmf\\anaconda3\\envs\\ml_venv\\lib\\site-packages\\torch\\hub.py:727\u001b[0m, in \u001b[0;36mload_state_dict_from_url\u001b[1;34m(url, model_dir, map_location, progress, check_hash, file_name)\u001b[0m\n\u001b[0;32m    725\u001b[0m         r \u001b[38;5;241m=\u001b[39m HASH_REGEX\u001b[38;5;241m.\u001b[39msearch(filename)  \u001b[38;5;66;03m# r is Optional[Match[str]]\u001b[39;00m\n\u001b[0;32m    726\u001b[0m         hash_prefix \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 727\u001b[0m     \u001b[43mdownload_url_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcached_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location)\n",
      "File \u001b[1;32mc:\\Users\\dhsmf\\anaconda3\\envs\\ml_venv\\lib\\site-packages\\torch\\hub.py:615\u001b[0m, in \u001b[0;36mdownload_url_to_file\u001b[1;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mfile_size, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m progress,\n\u001b[0;32m    613\u001b[0m           unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m, unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, unit_divisor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 615\u001b[0m         buffer \u001b[38;5;241m=\u001b[39m \u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    616\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    617\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhsmf\\anaconda3\\envs\\ml_venv\\lib\\http\\client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 463\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhsmf\\anaconda3\\envs\\ml_venv\\lib\\http\\client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    502\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\dhsmf\\anaconda3\\envs\\ml_venv\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhsmf\\anaconda3\\envs\\ml_venv\\lib\\ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1240\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1241\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\dhsmf\\anaconda3\\envs\\ml_venv\\lib\\ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1099\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1100\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# \n",
    "feature_extractor = fe.FeatureExtractor(model_name='resnet18', pretrained=True)\n",
    "\n",
    "# Autoencoder 모델 생성\n",
    "autoencoder = fe.Autoencoder(input_dim=512, encoding_dim=256).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.1449\n",
      "Epoch 2/100, Loss: 0.0858\n",
      "Epoch 3/100, Loss: 0.0855\n",
      "Epoch 4/100, Loss: 0.0850\n",
      "Epoch 5/100, Loss: 0.0849\n",
      "Epoch 6/100, Loss: 0.0857\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 7/100, Loss: 0.0851\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 8/100, Loss: 0.0856\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 9/100, Loss: 0.0851\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 10/100, Loss: 0.0852\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 11/100, Loss: 0.0854\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 12/100, Loss: 0.0862\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 13/100, Loss: 0.0849\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 14/100, Loss: 0.0854\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 15/100, Loss: 0.0852\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 16/100, Loss: 0.0860\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 17/100, Loss: 0.0852\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 18/100, Loss: 0.0852\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 19/100, Loss: 0.0858\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 20/100, Loss: 0.0853\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 21/100, Loss: 0.0846\n",
      "Epoch 22/100, Loss: 0.0851\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 23/100, Loss: 0.0854\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 24/100, Loss: 0.0857\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 25/100, Loss: 0.0853\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 26/100, Loss: 0.0850\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 27/100, Loss: 0.0854\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 28/100, Loss: 0.0848\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 29/100, Loss: 0.0853\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 30/100, Loss: 0.0852\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 31/100, Loss: 0.0851\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 32/100, Loss: 0.0857\n",
      "Epoch 00032: reducing learning rate of group 0 to 1.0000e-05.\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 33/100, Loss: 0.0855\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 34/100, Loss: 0.0858\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 35/100, Loss: 0.0857\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 36/100, Loss: 0.0854\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 37/100, Loss: 0.0856\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 38/100, Loss: 0.0852\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 39/100, Loss: 0.0859\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 40/100, Loss: 0.0852\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 41/100, Loss: 0.0849\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 42/100, Loss: 0.0853\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 43/100, Loss: 0.0853\n",
      "Epoch 00043: reducing learning rate of group 0 to 1.0000e-06.\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 44/100, Loss: 0.0857\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 45/100, Loss: 0.0853\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 46/100, Loss: 0.0849\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 47/100, Loss: 0.0856\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 48/100, Loss: 0.0856\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 49/100, Loss: 0.0857\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 50/100, Loss: 0.0851\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 51/100, Loss: 0.0849\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 52/100, Loss: 0.0857\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 53/100, Loss: 0.0853\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 54/100, Loss: 0.0854\n",
      "Epoch 00054: reducing learning rate of group 0 to 1.0000e-07.\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 55/100, Loss: 0.0856\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 56/100, Loss: 0.0857\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 57/100, Loss: 0.0849\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 58/100, Loss: 0.0847\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 59/100, Loss: 0.0854\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 60/100, Loss: 0.0850\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 61/100, Loss: 0.0859\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 62/100, Loss: 0.0851\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 63/100, Loss: 0.0851\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 64/100, Loss: 0.0854\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 65/100, Loss: 0.0859\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.0000e-08.\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 66/100, Loss: 0.0855\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 67/100, Loss: 0.0850\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 68/100, Loss: 0.0858\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 69/100, Loss: 0.0852\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 70/100, Loss: 0.0857\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 71/100, Loss: 0.0850\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 72/100, Loss: 0.0856\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 73/100, Loss: 0.0858\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 74/100, Loss: 0.0849\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 75/100, Loss: 0.0852\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 76/100, Loss: 0.0849\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 77/100, Loss: 0.0849\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 78/100, Loss: 0.0854\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 79/100, Loss: 0.0857\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 80/100, Loss: 0.0858\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 81/100, Loss: 0.0855\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 82/100, Loss: 0.0857\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 83/100, Loss: 0.0847\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 84/100, Loss: 0.0857\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 85/100, Loss: 0.0857\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 86/100, Loss: 0.0852\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 87/100, Loss: 0.0851\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 88/100, Loss: 0.0861\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 89/100, Loss: 0.0852\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 90/100, Loss: 0.0849\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 91/100, Loss: 0.0849\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 92/100, Loss: 0.0849\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 93/100, Loss: 0.0849\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 94/100, Loss: 0.0856\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 95/100, Loss: 0.0855\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 96/100, Loss: 0.0860\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 97/100, Loss: 0.0849\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 98/100, Loss: 0.0852\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 99/100, Loss: 0.0851\n",
      "No improvement in loss. Adjusting learning rate...\n",
      "Epoch 100/100, Loss: 0.0852\n",
      "No improvement in loss. Adjusting learning rate...\n"
     ]
    }
   ],
   "source": [
    "def train_autoencoder(model, train_loader, criterion, optimizer, scheduler, num_epochs=100):\n",
    "    model.train()\n",
    "    best_loss = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # 특징 추출\n",
    "            features = feature_extractor(images)\n",
    "            # Autoencoder를 통한 재구성\n",
    "            reconstructed = model(features)\n",
    "            # 재구성 손실 계산\n",
    "            loss = criterion(reconstructed, features)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # 평균 손실 계산\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "        \n",
    "        # scheduler.step()에 평균 손실 전달\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        # Early stopping 조건 확인\n",
    "        if best_loss is None or avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "        else:\n",
    "            print(\"No improvement in loss. Adjusting learning rate...\")\n",
    "            # 학습률 조정 로그는 ReduceLROnPlateau의 verbose=True를 통해 출력됩니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Autoencoder 모델, 손실 함수, 옵티마이저 설정\n",
    "autoencoder_criterion = nn.MSELoss()\n",
    "autoencoder_optimizer = optim.AdamW(autoencoder.parameters(), lr=0.001)\n",
    "autoencoder_scheduler = ReduceLROnPlateau(autoencoder_optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "\n",
    "# Autoencoder 모델 학습\n",
    "train_autoencoder(autoencoder, train_loader, autoencoder_criterion, autoencoder_optimizer,autoencoder_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculate_losses(train_loader, model):\n",
    "    model.eval()  # 평가 모드 전환\n",
    "    losses = []  # 손실 초기화\n",
    "    with torch.no_grad():\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "            features = feature_extractor(images)\n",
    "            reconstructed = model(features)\n",
    "            loss = nn.MSELoss(reduction='none')(reconstructed, features).mean(1)  # 손실 계산\n",
    "            losses.extend(loss.cpu().numpy())\n",
    "    return losses\n",
    "\n",
    "# 학습 데이터의 손실 계산\n",
    "losses = calculate_losses(train_loader, autoencoder)\n",
    "\n",
    "# 손실 분포 시각화\n",
    "plt.hist(losses, bins=50)\n",
    "plt.xlabel('Loss')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Losses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold for anomaly detection: 0.10806367546319959\n"
     ]
    }
   ],
   "source": [
    "# 손실 값의 상위 95%에 해당하는 분위수를 임계값으로 설정\n",
    "threshold = np.quantile(losses, 0.95)\n",
    "print(f\"Threshold for anomaly detection: {threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  label\n",
      "0  TEST_000      0\n",
      "1  TEST_001      0\n",
      "2  TEST_002      0\n",
      "3  TEST_003      0\n",
      "4  TEST_004      0\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터에 대해 이상 탐지 수행\n",
    "test_data = CustomDataset(csv_file='./test.csv', transform=transform)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# 제출 파일 로드\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "\n",
    "def detect_anomalies(loader, model, threshold):\n",
    "    model.eval()\n",
    "    anomaly_scores = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in loader:\n",
    "            images = images.to(device)\n",
    "            features = feature_extractor(images)\n",
    "            reconstructed = model(features)\n",
    "            loss = torch.mean((features - reconstructed) ** 2, dim=1)\n",
    "            anomaly_scores.extend(loss.cpu().numpy())\n",
    "    return anomaly_scores\n",
    "\n",
    "\n",
    "\n",
    "# 이상 탐지 수행 및 anomaly_scores 계산\n",
    "anomaly_scores = detect_anomalies(test_loader, autoencoder, threshold)\n",
    "\n",
    "# anomaly_scores를 사용하여 정상(0) 또는 이상(1) 레이블 결정\n",
    "submit['label'] = [1 if score > threshold else 0 for score in anomaly_scores]\n",
    "\n",
    "# 변경된 submit DataFrame 확인\n",
    "print(submit.head())\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "submit.to_csv('.result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
